{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Achillesy/ASPNET_MvcMusicStore/blob/master/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2nERgYezw1A",
      "metadata": {
        "id": "A2nERgYezw1A"
      },
      "source": [
        "# DeepTICI\n",
        "---\n",
        "This repository  is a fork of <https://github.com/IPMI-ICNS-UKE/DeepTICI>.\n",
        "Thanks to STROKE [publication](https://doi.org/10.1161/STROKEAHA.120.033807) for sharing the official code and model weights.\n",
        "\n",
        "The following code follows the license rights and limitations of the original shared code (CC BY-NC 4.0). You can view the full license text here:\n",
        "<https://raw.githubusercontent.com/IPMI-ICNS-UKE/DeepTICI/main/LICENSE.txt>\n",
        "\n",
        "\n",
        "**This code is provided for research and educational purposes only. Please do not use it for commercial purposes.**\n",
        "\n",
        "If you have any questions, please contact me at: xuchu_liu@rush.edu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bY_KC1wK89Eg",
      "metadata": {
        "id": "bY_KC1wK89Eg"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3euizQ2r9BUX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3euizQ2r9BUX",
        "outputId": "f52dfef5-6530-4303-8086-042b2079700c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.10/dist-packages (2.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet_pytorch\n",
        "!pip install SimpleITK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BPOj6eTT9VjY",
      "metadata": {
        "id": "BPOj6eTT9VjY"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper"
      ],
      "metadata": {
        "id": "h99Q81kdy5HT"
      },
      "id": "h99Q81kdy5HT"
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum, auto\n",
        "\n",
        "\"\"\"implements output and model modes to prevent using strings\"\"\"\n",
        "\n",
        "\n",
        "class OutputMode(Enum):\n",
        "    last_frame = auto()\n",
        "    all_frames = auto()\n",
        "\n",
        "\n",
        "class ModelMode(Enum):\n",
        "    train = auto()\n",
        "    inference = auto()\n"
      ],
      "metadata": {
        "id": "l3XJbH96zG_r"
      },
      "id": "l3XJbH96zG_r",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader"
      ],
      "metadata": {
        "id": "9jIrKWq-zQRC"
      },
      "id": "9jIrKWq-zQRC"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from abc import ABC\n",
        "from typing import List, Tuple, Union, Literal\n",
        "\n",
        "import SimpleITK as sitk\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from skimage.filters import threshold_multiotsu\n",
        "\n",
        "\n",
        "class BaseDataProcessor(ABC):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"initializes a chain of preprocessor steps which is then executed for each elem\"\"\"\n",
        "        self._chain = []\n",
        "\n",
        "    def _chain_exe(self, image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"applies the preprocessor steps to an image, based on the order in the chain\n",
        "            arg: image: nd array\n",
        "            return: image as nd array\"\"\"\n",
        "        for processor_step, kwargs in self._chain:\n",
        "            if kwargs:\n",
        "                image = processor_step(image, **kwargs)\n",
        "            else:\n",
        "                image = processor_step(image)\n",
        "        return image.astype(np.float32)\n",
        "\n",
        "    def add_processor_step(self, func_name: str, kwargs: dict):\n",
        "        \"\"\"appends preprocessor step from name and kwargs to the chain\n",
        "        args:\n",
        "            func_name: name of the preprocessor function\n",
        "            kwargs: keyword arguments applied to the func_name together with nd image/array\"\"\"\n",
        "        func = getattr(self, func_name)\n",
        "        self._chain.append((func, kwargs))\n",
        "\n",
        "    @classmethod\n",
        "    def to_chain(cls, chain_elems: dict):\n",
        "        \"\"\"adds preprocesser functions to the chain with corresponding elements.\n",
        "        chain_elems contains the functain name as a key and kwargs as values\n",
        "        args:\n",
        "            chain_elems: is a dictionary with the preprocessor function name as key and the coressponding kwargs as\n",
        "            items. Items are a dict itself.\"\"\"\n",
        "        inst = cls()\n",
        "        for chain_elem, elem_kwargs in chain_elems.items():\n",
        "            inst.add_processor_step(chain_elem, elem_kwargs)\n",
        "        return inst\n",
        "\n",
        "    def __call__(self, image: np.ndarray, **kwargs):\n",
        "        \"\"\"executes chain\"\"\"\n",
        "        return self._chain_exe(image)\n",
        "\n",
        "\n",
        "class BasePreProcessor(BaseDataProcessor):\n",
        "    \"\"\"Implementation of Abstract Class BaseDataProcessor. Basic steps are implemented for preprocessing.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    \"\"\"all functions take an nd.array as input and return a n.array\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def temp_normalize_image(image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"normalizes images by substracting the mean along temporal axis\"\"\"\n",
        "        return image - np.mean(image, axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_image_range(image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"scales image values to an intervall of -1 to 1\"\"\"\n",
        "        return 2 * (image - image.min()) / (image.max() - image.min()) - 1\n",
        "\n",
        "    @staticmethod\n",
        "    def median_normalize_slice(image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"sets the image median to a value of 0\"\"\"\n",
        "        return image - np.median(image, axis=(-2, -1), keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def set_background_to_one(image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"sets background pixel value to one\"\"\"\n",
        "        if image.sum() / np.prod(image.shape) < 0:\n",
        "            image = image * -1\n",
        "        return image\n",
        "\n",
        "\n",
        "class PreProcessor(BasePreProcessor):\n",
        "    \"\"\"implements more specicfic PreProcessor steps, that take additional arguments\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clip_image(image: np.ndarray, boundaries: Tuple[float, float] = None,\n",
        "                   mode: Literal['multiotsu', 'median'] = None) -> np.ndarray:\n",
        "        \"\"\"clips image values to an intervall based on image intensieties.\n",
        "        Availabe are multiotsu thresholding, and above median\n",
        "        args:\n",
        "            boundaries: image is clipped by those fixed values, must be in the format(min,max). If mode is given,\n",
        "                        boundaries are ignored.\n",
        "            mode: must be either mutlitotsu or median. If multiotsu boundraries are based on two thresholds. If median\n",
        "                  the upper boundary is the median value and lower the min value of the image \"\"\"\n",
        "\n",
        "        if mode == 'multiotsu':\n",
        "            boundaries = threshold_multiotsu(image, classes=3)\n",
        "        elif mode == 'median':\n",
        "            boundaries = [image.min(), None]\n",
        "            boundaries[1] = np.median(image)\n",
        "        elif not boundaries and not mode:\n",
        "            boundaries = [image.min(), image.max()]\n",
        "        return np.clip(image, boundaries[0], boundaries[-1])\n",
        "\n",
        "    @staticmethod\n",
        "    def resample_image(image: np.ndarray, frame_shape=(380, 380)) -> np.ndarray:\n",
        "        \"\"\"resamples image to specified pixel size using nearest neighbours\n",
        "        args:\n",
        "            frame_shape: crops the last two dimensions to the given value\"\"\"\n",
        "        num_frames = len(image)\n",
        "        resized_image = np.zeros((num_frames,) + frame_shape)\n",
        "        for idx, frame in enumerate(image):\n",
        "            resized_image[idx] = cv2.resize(frame, frame_shape, interpolation=cv2.INTER_NEAREST)\n",
        "        return resized_image\n",
        "\n",
        "    @staticmethod\n",
        "    def crop_image(image: np.ndarray, tol=0.01) -> np.ndarray:\n",
        "        \"\"\"removes bars (with noise) on image edges. tol is the amount of noise allowed.\n",
        "            args:\n",
        "                tol: tolerance value above which axis are accepted to contain image information. If a whole row\n",
        "                     contains no information and is at the image edge it is assumed to contain no value for\n",
        "                     classifiation\"\"\"\n",
        "        mask = np.std(image, axis=0) > tol\n",
        "        idx = np.ix_(mask.any(1), mask.any(0))\n",
        "        return image[:, idx[0], idx[1]]\n",
        "\n",
        "    @staticmethod\n",
        "    def center_crop(image, rel_size=(0.5, 0.5)) -> np.ndarray:\n",
        "        \"\"\"takes the center crop of a given image. rel_size is the relative size of the center crop in realtion to the\n",
        "         input image\n",
        "         args:\n",
        "            rel_size: relative size of the center crop in respect to the orginal image. \"\"\"\n",
        "        x_size, y_size = image.shape[-2:]\n",
        "        pos_x_1 = int(x_size * (1 - rel_size[0]) / 2)\n",
        "        pos_y_1 = int(y_size * (1 - rel_size[1]) / 2)\n",
        "        pos_x_2 = int(pos_x_1 + x_size * rel_size[0])\n",
        "        pos_y_2 = int(pos_y_1 + y_size * rel_size[1])\n",
        "        image = image[..., pos_x_1:pos_x_2, pos_y_1:pos_y_2]\n",
        "        return image\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, preprocessor_steps: dict):\n",
        "        \"\"\"class to load dicom images from path, gets preprocessor arguments to build the preprocessor\n",
        "        args:\n",
        "            preprocessor_steps: dict containing names and kwargs of the preprocessing steps\"\"\"\n",
        "        self.preprocessor = PreProcessor.to_chain(preprocessor_steps)\n",
        "\n",
        "    def __call__(self, series_paths: List[Union[str, os.PathLike]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"loads images to the RAM and applies the preprocessor steps\n",
        "        args:\n",
        "            series_paths: List of paths. First entry must point to AP and second to the lateral view. Views must be the\n",
        "                          same the length.\n",
        "            return: A tuple containing, two tensors 1.) both views shape: 1 (dummy batch) x time x 2 (views) x height x\n",
        "                    width 2.) length of the series \"\"\"\n",
        "\n",
        "        # loads both dicom views\n",
        "        series = [self._load_image(path) for path in series_paths]\n",
        "        series_lengths = [len(view) for view in series]\n",
        "        assert len(series_lengths) == 2, f'expected 2 views, got {len(series_lengths)} views'\n",
        "        assert series_lengths[0] == series_lengths[1], f'Length of DSA views are not equal'\n",
        "\n",
        "        # converts views to array and make a 3Ch image. The 3Ch are filled with consecutive frames\n",
        "        series = np.array(series)\n",
        "        series = self._make_3_ch_img(series)\n",
        "        series = torch.tensor(series, dtype=torch.float32).unsqueeze(dim=0)\n",
        "        return series, torch.tensor(series_lengths[0]).unsqueeze(dim=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_3_ch_img(img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"creates a 3Ch image from a 1Ch image. The 3Ch consists of 3 consecutive frames, centered around the original\n",
        "        Frame\n",
        "        arg:\n",
        "            img: 1CH Image\n",
        "        return: same image with 3CH. Each slice from the orginal image is converted to the center channel sourounded by\n",
        "                the previous and nect slice.\"\"\"\n",
        "        new_img = np.zeros((*img.shape[0:2], 3, *img.shape[-2:])).swapaxes(1, 0)\n",
        "        for i_frame in range(len(new_img)):\n",
        "            img_temp = np.zeros(new_img.shape[1:])\n",
        "            for i_channel in range(img_temp.shape[1]):\n",
        "                if i_frame == 0 and i_channel == 0:\n",
        "                    pass\n",
        "                elif i_frame == len(new_img) - 1 and i_channel == 2:\n",
        "                    pass\n",
        "                else:\n",
        "                    img_temp[:, i_channel] = img[:, i_frame - 1 + i_channel]\n",
        "            new_img[i_frame] = img_temp\n",
        "        return new_img\n",
        "\n",
        "    def _load_image(self, image_path: Union[str, os.PathLike]) -> np.ndarray:\n",
        "        \"\"\"loads dicom images to RAM and preprocesses image from path\n",
        "            args:\n",
        "                image_paths: list containing path to two views order: [ap, lat]\n",
        "            return: preprocessed dicom image as np.array\"\"\"\n",
        "        image = self._load_img_to_ram(image_path)\n",
        "        image = self.preprocessor(image)\n",
        "        return image.astype(np.half)\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_img_to_ram(file_path: Union[str, os.PathLike]) -> np.ndarray:\n",
        "        \"\"\"loads dicom image to RAM\n",
        "            args:\n",
        "                image_path: path pointing to an dicom image\n",
        "            return: dicom image as np.array\"\"\"\n",
        "        assert file_path.endswith('.dcm'), 'only Dicom file format is supported'\n",
        "        image = sitk.GetArrayFromImage(sitk.ReadImage(file_path))\n",
        "        return image\n"
      ],
      "metadata": {
        "id": "JH6GUqGRzoO5"
      },
      "id": "JH6GUqGRzoO5",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "KsBNsPzk09PT"
      },
      "id": "KsBNsPzk09PT"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "from typing import Union, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "class SwishImplementation(torch.autograd.Function):\n",
        "    \"\"\"legacy method used before torch.nn.SiLU was available\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        result = i * torch.sigmoid(i)\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        i = ctx.saved_tensors[0]\n",
        "        sigmoid_i = torch.sigmoid(i)\n",
        "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"pytorch implementation of Swish activation function\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return SwishImplementation.apply(x)\n",
        "\n",
        "\n",
        "class EfficientTwoArmEncoder(nn.Module):\n",
        "    \"\"\"Implements a two-arm-encoder network based on an efficient-net b0 backbone. This networks fuses each frame of two\n",
        "     views to one latent representation by combining them before the last convolutional layer.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 7, in_channels: int = 3, feature_size: int = 1280):\n",
        "        \"\"\"args:\n",
        "            num_classes: Number of classes if this network is used for classification.\n",
        "            in_channels: Number of channels in the input image\n",
        "            feature_size: size of the latent space\"\"\"\n",
        "        super().__init__()\n",
        "        # Efficient-net backbone\n",
        "        self.feature_extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,\n",
        "                                                              in_channels=in_channels)\n",
        "        self.swish = Swish()\n",
        "\n",
        "        # combines latent representation of the two views\n",
        "        self.combine_layer = nn.Sequential(\n",
        "            nn.Conv2d(320 * 2, feature_size, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_size, momentum=0.01, eps=1e-3),\n",
        "            self.swish\n",
        "        )\n",
        "\n",
        "        # classification layer\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Linear(feature_size, num_classes),\n",
        "            Swish(),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        \"\"\"args:\n",
        "            x: Tensor of size batch_size x 2 (num_views) x frames x channels x height x width\n",
        "            returns: latent representation of x batch_size x frames x feature_size\"\"\"\n",
        "\n",
        "        x_shape = x.shape\n",
        "        # flatten image to batch_size*views*frames x channels x height x width\n",
        "        x = x.contiguous().view((-1, *x_shape[-3:]))\n",
        "\n",
        "        # encode all frames and views simultaneously\n",
        "        x = self.extract_features(x)\n",
        "\n",
        "        # unflatten to dimension: batch_size*frames x channels*views x height x width\n",
        "        x = x.view((-1, x.shape[-3] * 2, *x.shape[-2:]))\n",
        "        x = self.combine_layer(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = x.squeeze(dim=2).squeeze(dim=2)\n",
        "\n",
        "        # reshape to batch_size x frames x feature_size\n",
        "        x = x.view((*x_shape[:2], x.shape[-1]))\n",
        "        return x\n",
        "\n",
        "    def extract_features(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"applies all layers of efficient-net b0 until the last convolution\n",
        "        args:\n",
        "            x: input tensor shape: n x channels x height x width\"\"\"\n",
        "        x = self.swish(self.feature_extractor._bn0(self.feature_extractor._conv_stem(x)))\n",
        "        for idx, block in enumerate(self.feature_extractor._blocks):\n",
        "            drop_connect_rate = self.feature_extractor._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self.feature_extractor._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FastGRU(nn.Module):\n",
        "    def __init__(self, feature_size: int, output_size: int = None):\n",
        "        \"\"\"intializes weights of the GRU with three gates: Input, forget and hidden.\n",
        "        args:\n",
        "            feature_size: Size of the input in the last dimension\n",
        "            output_size: size of the cell state/ output. if not given it is equal to feature_size\"\"\"\n",
        "        super().__init__()\n",
        "        self.input_size = feature_size\n",
        "        if not output_size:\n",
        "            self.hidden_size = self.input_size\n",
        "        else:\n",
        "            self.hidden_size = output_size\n",
        "        self.W = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size * 3))\n",
        "        self.U = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size * 3))\n",
        "        self.bias = nn.Parameter(torch.Tensor(self.hidden_size * 3))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"implements GRU forward pass with three gates. Input, forget and output. The input gate controls the relevance\n",
        "         of new timesteps, the forget gate the relevance of the cell state in regard to the new input and the output\n",
        "         gate controls the new cellstate.\n",
        "         args:\n",
        "            x: input tensor of shape batch x timesteps x feature_size\"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        hidden_seq = []\n",
        "        dropout_prob = 0.1\n",
        "        h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                    torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "\n",
        "        x = F.dropout(input=x, p=dropout_prob, training=self.training)\n",
        "\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            gates = x_t @ self.U + c_t @ self.W + self.bias\n",
        "\n",
        "            z_t, r_t = (\n",
        "                # input\n",
        "                torch.sigmoid(gates[:, :self.hidden_size]),\n",
        "                # forget\n",
        "                torch.sigmoid(gates[:, self.hidden_size:self.hidden_size * 2])\n",
        "            )\n",
        "\n",
        "            x_t = torch.tanh(r_t * x_t @ self.U + c_t @ self.W + self.bias)[:,\n",
        "                  self.hidden_size * 2:self.hidden_size * 3]\n",
        "\n",
        "            c_t = (1 - z_t) * c_t + x_t\n",
        "\n",
        "            hidden_seq.append(c_t.unsqueeze(0))\n",
        "\n",
        "            c_t = F.dropout(input=c_t, p=dropout_prob, training=self.training)\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        return hidden_seq\n",
        "\n",
        "\n",
        "class TICIModelHandler(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes: int,\n",
        "                 feature_size: int,\n",
        "                 pretrained: Union[None, Union[str, os.PathLike], List[Union[str, os.PathLike]]] = None,\n",
        "                 in_channels: int = 3,\n",
        "                 output_size: int = None\n",
        "                 ):\n",
        "        \"\"\"Wrapper around Encoder+GRU+Classifier structure. Serves model loading and selecting the right timesteps for\n",
        "        training and inference.\n",
        "        args:\n",
        "            num_classes: number of classes in the output\n",
        "            feature_size: output size of the encoder network\n",
        "            pretrained: path(s) to pickeld weights, if multiple paths are given an ensamble is applied\n",
        "            in_channels: expected number of channels in the input image\n",
        "            output_size: size of the cell state for the GRU. If not given equal to feature_size\"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.network = TICITemporalNetwork(in_channels=in_channels, feature_size=feature_size, output_size=output_size,\n",
        "                                           num_classes=num_classes)\n",
        "        self.pretrained = False\n",
        "        if pretrained:\n",
        "            if isinstance(pretrained, list):\n",
        "                self.pretrained = pretrained\n",
        "            else:\n",
        "                # if multiple paths ensemble over all given weights\n",
        "                self.pretrained = True\n",
        "                self.load_model(pretrained)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        \"\"\"load state dict of the network.\n",
        "        args:\n",
        "            path: path to weights\"\"\"\n",
        "        state_dict = torch.load(path)\n",
        "        while any(['module.' in k for k in state_dict.keys()]):\n",
        "            # remove eventual pytorch wrapper\n",
        "            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "        self.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    @autocast()\n",
        "    def forward(self, x: torch.Tensor, series_length: torch.Tensor = None, model_mode: ModelMode = ModelMode.inference,\n",
        "                output_mode: OutputMode = OutputMode.last_frame) -> torch.tensor:\n",
        "        if x.dim() == 5:\n",
        "            x = x.unsqueeze(dim=0)\n",
        "        assert x.dim() == 6, 'Input must be 4 or 5 dimensional ((batch) x time x view x channels x height x width'\n",
        "        if series_length:\n",
        "            if series_length.dim() == 0:\n",
        "                series_length = series_length.unsqueeze(dim=0)\n",
        "\n",
        "        series_length = series_length.to(x.device)\n",
        "        assert isinstance(model_mode, ModelMode), 'forward pass mode is not implemented (yet)'\n",
        "\n",
        "        # inference\n",
        "        if model_mode == ModelMode.inference:\n",
        "            with torch.no_grad():\n",
        "                # ensemble\n",
        "                if isinstance(self.pretrained, list):\n",
        "                    for path in self.pretrained:\n",
        "                        self.load_model(path)\n",
        "                        self.eval()\n",
        "                        try:\n",
        "                            predictions += self.network(x)\n",
        "                        except NameError:\n",
        "                            predictions = self.network(x)\n",
        "                    predictions /= len(self.pretrained)\n",
        "                else:\n",
        "                    self.eval()\n",
        "                    predictions = self.network(x)\n",
        "        # training\n",
        "        elif model_mode == ModelMode.train:\n",
        "            self.train()\n",
        "            predictions = self.network(x)\n",
        "        else:\n",
        "            raise NotImplementedError('forward pass mode is not implemented (yet)')\n",
        "\n",
        "        assert isinstance(output_mode, OutputMode), 'output mode is not implemented (yet)'\n",
        "        if output_mode == OutputMode.last_frame:\n",
        "            predictions = self._get_last_frame_in_batch(predictions, series_length)\n",
        "        elif output_mode == OutputMode.all_frames:\n",
        "            pass\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_last_frame_in_batch(predictions: torch.Tensor, series_length: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"iterates through batch to fetch the last frame of each input\n",
        "        args:\n",
        "            predictions: tensor of shape batch x time x num_classes\n",
        "            series_length: tensor containing the series lengths of batch elements. If not specified the last frame is\n",
        "            considered\"\"\"\n",
        "        if not series_length:\n",
        "            pred = predictions[:, -1]\n",
        "        else:\n",
        "            pred = torch.zeros((predictions.shape[0], predictions.shape[-1]), device=predictions.device)\n",
        "            for i, _series_length in enumerate(series_length):\n",
        "                pred[i] = predictions[i, _series_length - 1]\n",
        "        return pred\n",
        "\n",
        "\n",
        "class TICITemporalNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 output_size: int,\n",
        "                 num_classes: int,\n",
        "                 feature_size: int) -> torch.Tensor:\n",
        "        \"\"\"Wraps around encoder, GRU and classifier.\n",
        "        args:\n",
        "            in_channels: number of channels in input\n",
        "            output_size: size of the GRUs cell state\n",
        "            num_classes: number of classes in the dataset\n",
        "            feature_size: number of features from the encoder\"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = EfficientTwoArmEncoder(feature_size=feature_size, in_channels=in_channels,\n",
        "                                              num_classes=num_classes)\n",
        "        self.gru = FastGRU(feature_size=feature_size, output_size=output_size)\n",
        "        self.classifier = nn.Sequential(nn.Dropout(),\n",
        "                                        nn.Linear(feature_size, num_classes),\n",
        "                                        Swish(),\n",
        "                                        nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"forward pass through all model parts:\n",
        "        args:\n",
        "            x: input tensor of shape batch x time x views x channels x height x width\"\"\"\n",
        "        x = self.encoder(x, mode='encoder')\n",
        "        x = self.gru(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-w-p_T570irE"
      },
      "id": "-w-p_T570irE",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "A9YxtrrO1KVF"
      },
      "id": "A9YxtrrO1KVF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load DSA series pipeline"
      ],
      "metadata": {
        "id": "5elwcbCV3gKJ"
      },
      "id": "5elwcbCV3gKJ"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor_steps = {\n",
        "    'crop_image': {'tol': 0.01},\n",
        "    'clip_image': {'mode': 'multiotsu'},\n",
        "    'temp_normalize_image': None,\n",
        "    'normalize_image_range': None,\n",
        "    'resample_image': {\n",
        "        'frame_shape': (224, 224)\n",
        "    }\n",
        "}\n",
        "data_loader = DataLoader(preprocessor_steps)"
      ],
      "metadata": {
        "id": "CrJ9N4Gp4n1p"
      },
      "id": "CrJ9N4Gp4n1p",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model weights"
      ],
      "metadata": {
        "id": "A1HkOpDl4oDZ"
      },
      "id": "A1HkOpDl4oDZ"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/IPMI-ICNS-UKE/DeepTICI/raw/main/model_weights/experiment_I"
      ],
      "metadata": {
        "id": "VCgJcMxr0iwT",
        "outputId": "eaa2faaf-513e-4614-cb3b-8c878ca14bea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VCgJcMxr0iwT",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-13 20:35:42--  https://github.com/IPMI-ICNS-UKE/DeepTICI/raw/main/model_weights/experiment_I\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/IPMI-ICNS-UKE/DeepTICI/main/model_weights/experiment_I [following]\n",
            "--2023-12-13 20:35:42--  https://raw.githubusercontent.com/IPMI-ICNS-UKE/DeepTICI/main/model_weights/experiment_I\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59091797 (56M) [application/octet-stream]\n",
            "Saving to: ‘experiment_I’\n",
            "\n",
            "experiment_I        100%[===================>]  56.35M   287MB/s    in 0.2s    \n",
            "\n",
            "2023-12-13 20:35:46 (287 MB/s) - ‘experiment_I’ saved [59091797/59091797]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "    'pretrained': 'experiment_I',\n",
        "    'feature_size': 1280,\n",
        "    'num_classes': 6,\n",
        "    'in_channels': 3,\n",
        "    'output_size': 1280\n",
        "}\n",
        "model = TICIModelHandler(**model_params)"
      ],
      "metadata": {
        "id": "Xt_SlMPT0iz3",
        "outputId": "09ea168c-93af-4147-a3ac-2fdbe153e340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Xt_SlMPT0iz3",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n",
            "100%|██████████| 20.4M/20.4M [00:00<00:00, 360MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "L2FtGExY05BR"
      },
      "id": "L2FtGExY05BR"
    },
    {
      "cell_type": "markdown",
      "id": "dqPv6Jms-Udn",
      "metadata": {
        "id": "dqPv6Jms-Udn"
      },
      "source": [
        "### Please upload your files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![upload_guide](https://github.com/Achillesy/Fetal_Functional_MRI_Segmentation/blob/master/figures/upload_guide.png?raw=1)\n",
        "1. Click the **Files** icon on the left\n",
        "2. Click the **Upload to session storage** icon above\n",
        "3. Your uploaded files will be displayed here\n",
        "----\n",
        "After double-checking your uploaded files, by <font color=\"green\">pressing the **Enter** key in the input box below</font>, the fMRI mask will be automatically generated in a short time."
      ],
      "metadata": {
        "id": "GD0eE3A57Jff"
      },
      "id": "GD0eE3A57Jff"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "26b1a6e0-4b17-40a4-a45c-632546836c0a",
      "metadata": {
        "id": "26b1a6e0-4b17-40a4-a45c-632546836c0a",
        "outputId": "972c2b48-5caf-4738-b47d-2f496aeff98a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "input()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RFxygYdW5f9E"
      },
      "id": "RFxygYdW5f9E",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0a99e9bc-f612-47ab-a113-184b024170b1",
      "metadata": {
        "id": "0a99e9bc-f612-47ab-a113-184b024170b1",
        "outputId": "2ad7c2c0-f99f-484f-f534-887591cef9b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3: 'TICI 2b'\n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "\n",
        "label_mapping = {\n",
        "    0: 'TICI 0',\n",
        "    1: 'TICI 0',\n",
        "    2: 'TICI 2a',\n",
        "    3: 'TICI 2b',\n",
        "    4: 'TICI 3',\n",
        "    5: 'TICI 3'\n",
        "}\n",
        "\n",
        "dcm_path = glob(\"*.dcm\")\n",
        "data = data_loader(dcm_path)\n",
        "raw_values = model(*data, model_mode=ModelMode.inference, output_mode=OutputMode.last_frame)\n",
        "score = [int(torch.argmax(raw_value, dim=-1)) for raw_value in raw_values]\n",
        "score = score[0]\n",
        "print(f\"{score}: '{label_mapping[score]}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7d899e7-2779-4c28-bf37-01e6471c7e51",
      "metadata": {
        "id": "a7d899e7-2779-4c28-bf37-01e6471c7e51"
      },
      "outputs": [],
      "source": [
        "import pydicom\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L0MKNDE0-e2o",
      "metadata": {
        "id": "L0MKNDE0-e2o"
      },
      "source": [
        "## Clean up temporary files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6d4a32d1-b747-47d4-81a5-f834fb1c94c7",
      "metadata": {
        "id": "6d4a32d1-b747-47d4-81a5-f834fb1c94c7"
      },
      "outputs": [],
      "source": [
        "!rm *.dcm"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}